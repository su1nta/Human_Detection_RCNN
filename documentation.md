# Real-time object detection and tracking using Faster RCNN and DeepSORT

### Table of contents
- Project Abstract
- Technologies used in the project
- Step by step implementation
- Result set (CSV format)
- Visualizing result
- Total time taken by algorithm

## Abstract
Real-time object detection and tracking have become vital in various fields such as surveillance, autonomous vehicles, and human-computer interaction. In this project, we present a system that integrates Faster RCNN (Region-based Convolutional Neural Network) for object detection and DeepSORT (Deep Simple Online and Realtime Tracking) for object tracking. The system is capable of accurately detecting and tracking objects in real-time video streams, distinguishing between still and moving objects, and recording their bounding box coordinates along with unique IDs in a CSV (Comma-Separated Values) file.

The Faster RCNN model is employed for its high accuracy in detecting objects within images, utilizing a region-based approach to identify and classify objects. Once objects are detected, DeepSORT is utilized to track these objects across frames, maintaining consistency in identification even in complex scenarios such as occlusions and temporary disappearances. Additionally, the system incorporates functionality to differentiate between stationary and moving objects, providing valuable insights into object behavior over time.

The output of the system is stored in a CSV file, containing the bounding box coordinates of detected objects along with their corresponding IDs. This structured format enables easy retrieval and analysis of object trajectories and behaviors, facilitating applications such as crowd monitoring, traffic analysis, and anomaly detection.

The implementation of this system offers a flexible and efficient solution for real-time object detection and tracking tasks, with potential applications in diverse domains including security, retail, and transportation

## Technologies and Algorithms used
- Languages and Libraries used:
    - Python
    - NumPy
    - OpenCV
    - PyTorch
    - TorchVision

- Algorithms Used:
    - Faster RCNN
    - DeepSORT

## Faster R-CNN

### Context:
Before Faster R-CNN (Region-based Convolutional Neural Network), traditional object detection methods relied on two-stage approaches where potential object regions were first proposed and then classified. However, these methods were often computationally expensive due to the need for external region proposal algorithms, such as Selective Search or EdgeBoxes.

### Introduction to Faster R-CNN:
Faster R-CNN, introduced by Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun in 2015, is a state-of-the-art object detection algorithm that integrates region proposal networks (RPNs) directly into the object detection pipeline. This eliminates the need for separate region proposal algorithms and significantly speeds up the detection process.

### Key Components of Faster R-CNN:

1. **Convolutional Backbone Network:**
   Faster R-CNN typically employs a convolutional neural network (CNN), such as VGG, ResNet, or similar architectures, as its backbone network. This network is responsible for extracting high-level features from the input image.

2. **Region Proposal Network (RPN):**
   The RPN is a lightweight neural network that operates on feature maps generated by the convolutional backbone. It generates region proposals, which are candidate bounding boxes likely to contain objects. These proposals are generated at various scales and aspect ratios across the feature map.

3. **Region of Interest (RoI) Pooling:**
   After generating region proposals, RoI pooling is applied to extract fixed-size feature vectors from each proposal. These feature vectors are then fed into a classifier and a bounding box regressor for object classification and localization.

4. **Classifier and Bounding Box Regressor:**
   The classifier assigns object class probabilities to each proposal, determining whether it contains an object and if so, which class it belongs to. Meanwhile, the bounding box regressor refines the coordinates of the proposed bounding boxes to better fit the object's true location.

### Workflow of Faster R-CNN:

1. **Feature Extraction:**
   The input image is passed through the convolutional backbone network, resulting in a feature map that captures hierarchical representations of the image.

2. **Region Proposal Generation:**
   The RPN operates on the feature map to generate a set of region proposals. Each proposal consists of a bounding box and a corresponding objectness score, indicating the likelihood of containing an object.

3. **Region Classification and Localization:**
   The proposed regions are subjected to RoI pooling, producing fixed-size feature vectors. These features are then fed into separate branches for object classification and bounding box regression.

4. **Post-processing:**
   Finally, non-maximum suppression (NMS) is applied to remove duplicate detections and refine the final set of detected objects based on their confidence scores.

### Advantages of Faster R-CNN:

- **Accuracy:** Faster R-CNN achieves high accuracy in object detection tasks due to its ability to learn discriminative features from both region proposals and the entire image.
- **Efficiency:** By integrating region proposal generation directly into the neural network, Faster R-CNN significantly reduces computational overhead compared to traditional two-stage methods.
- **Flexibility:** The modular architecture of Faster R-CNN allows for easy integration with various backbone networks and optimization techniques, making it adaptable to different domains and applications.


## DeepSORT

### Context:
Before DeepSORT (Deep Simple Online and Realtime Tracking), object tracking in video sequences often faced challenges such as maintaining identity consistency across frames, handling occlusions, and dealing with varying object appearances. Traditional tracking methods like Kalman filters or simple association techniques struggled with these complexities.

### Introduction to DeepSORT:
DeepSORT, introduced by Wojciech Zaremba and Alex Bewley in 2017, is an extension of the SORT (Simple Online and Realtime Tracking) algorithm that integrates deep learning techniques for object tracking. It combines the advantages of a deep appearance descriptor with the simplicity and efficiency of the SORT algorithm to achieve state-of-the-art performance in multi-object tracking.

### Key Components of DeepSORT:

1. **Detection Module:**
   DeepSORT begins by detecting objects in each frame of a video sequence using a pre-trained object detection model such as Faster R-CNN or YOLO. These detections provide bounding boxes and associated confidence scores for potential objects.

2. **Feature Extraction:**
   Once objects are detected, DeepSORT extracts appearance features from each object's bounding box using a deep neural network. These appearance features encode the visual characteristics of each object and are crucial for associating objects across frames.

3. **Data Association:**
   DeepSORT employs a data association algorithm to link object detections across consecutive frames and maintain identity consistency. It utilizes both motion information (e.g., predicted object positions based on Kalman filtering) and appearance features to match detections and assign unique IDs to tracked objects.

4. **Kalman Filtering:**
   To predict the state of each object (e.g., position and velocity) between frames and compensate for noisy detections, DeepSORT utilizes Kalman filters. Kalman filters provide estimates of object states based on previous observations and help maintain smooth and consistent trajectories.

5. **Track Management:**
   DeepSORT includes mechanisms for track initialization, termination, and updating. It initializes tracks for newly detected objects, removes tracks for objects that are no longer visible or reliably tracked, and updates existing tracks with new observations.

### Workflow of DeepSORT:

1. **Detection:**
   Object detections are obtained using a pre-trained object detection model, providing bounding boxes and confidence scores for objects in each frame.

2. **Feature Extraction:**
   Appearance features are extracted from the bounding boxes of detected objects using a deep neural network, encoding visual characteristics.

3. **Data Association:**
   Detections are associated across frames using a combination of motion prediction, appearance matching, and track management strategies to maintain identity consistency.

4. **Kalman Filtering:**
   Kalman filters predict the state of each tracked object between frames and help smooth out noisy detections.

5. **Track Management:**
   Tracks are initialized, updated, and terminated based on the availability and reliability of detections, ensuring consistent and accurate object tracking over time.

### Advantages of DeepSORT:

- **Robustness:** DeepSORT's combination of appearance features and motion information enhances robustness in tracking scenarios with occlusions, varying object appearances, and complex motion patterns.
- **Accuracy:** By leveraging deep learning techniques for appearance modeling, DeepSORT achieves high accuracy in multi-object tracking tasks, even in challenging environments.
- **Real-Time Performance:** DeepSORT maintains real-time performance, making it suitable for applications requiring fast and reliable object tracking in video streams.


## Project Workflow